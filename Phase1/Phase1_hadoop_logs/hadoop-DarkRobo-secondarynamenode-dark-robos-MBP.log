2017-09-08 16:36:11,603 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-08 16:36:11,633 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-08 16:36:12,347 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-08 16:36:12,570 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-08 16:36:12,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-08 16:36:12,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-08 16:36:13,011 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-08 16:36:13,114 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 3830@dark-robos-mbp
2017-09-08 16:36:13,140 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-08 16:36:13,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-08 16:36:13,145 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-08 16:36:13,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-08 16:36:13,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-08 16:36:13,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-08 16:36:13,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 08 16:36:13
2017-09-08 16:36:13,208 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-08 16:36:13,208 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-08 16:36:13,210 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-08 16:36:13,210 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-08 16:36:13,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-08 16:36:13,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-08 16:36:13,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-08 16:36:13,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-08 16:36:13,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-08 16:36:13,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-08 16:36:13,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-08 16:36:13,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-08 16:36:13,254 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-08 16:36:13,254 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-08 16:36:13,254 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-08 16:36:13,255 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-08 16:36:13,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-08 16:36:13,687 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-08 16:36:13,687 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-08 16:36:13,687 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-08 16:36:13,688 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-08 16:36:13,689 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-08 16:36:13,690 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-08 16:36:13,690 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-08 16:36:13,699 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-08 16:36:13,700 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-08 16:36:13,700 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-08 16:36:13,700 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-08 16:36:13,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-08 16:36:13,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-08 16:36:13,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-08 16:36:13,762 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-08 16:36:13,762 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-08 16:36:13,762 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-08 16:36:13,777 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-08 16:36:13,777 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-08 16:36:13,793 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-08 16:36:13,881 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-08 16:36:13,892 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-08 16:36:13,899 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-08 16:36:13,906 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-08 16:36:13,909 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-08 16:36:13,909 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-08 16:36:13,909 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-08 16:36:13,946 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-08 16:36:13,946 INFO org.mortbay.log: jetty-6.1.26
2017-09-08 16:36:14,340 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-08 16:36:14,340 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-08 16:42:15,105 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-09-08 16:42:15,457 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:1238799902:1504906503682:CID-dd6d673d-4019-46a0-9c71-d15d614fc6f8&bootstrapstandby=false
2017-09-08 16:42:15,549 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-09-08 16:42:15,778 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2017-09-08 16:42:15,778 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 325 bytes.
2017-09-08 16:42:15,802 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=4&storageInfo=-63:1238799902:1504906503682:CID-dd6d673d-4019-46a0-9c71-d15d614fc6f8
2017-09-08 16:42:15,807 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-09-08 16:42:15,807 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000004_0000000000003656449 size 0 bytes.
2017-09-08 16:42:15,897 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-09-08 16:42:15,932 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-09-08 16:42:15,933 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage_0000000000000000000
2017-09-08 16:42:15,933 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-09-08 16:42:15,948 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-09-08 16:42:15,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000004 expecting start txid #1
2017-09-08 16:42:15,956 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000004
2017-09-08 16:42:16,040 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000004 of size 201 edits # 4 loaded in 0 seconds
2017-09-08 16:42:16,058 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000004 using no compression
2017-09-08 16:42:16,107 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000004 of size 468 bytes saved in 0 seconds.
2017-09-08 16:42:16,111 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-DarkRobo/dfs/namesecondary
2017-09-08 16:42:16,121 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-DarkRobo/dfs/namesecondary
2017-09-08 16:42:16,242 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 4 to namenode at http://localhost:50070 in 0.12 seconds
2017-09-08 16:42:16,243 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 468
2017-09-08 17:00:15,895 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-08 17:00:15,906 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-10 13:45:13,561 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-10 13:45:13,620 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-10 13:45:14,255 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-10 13:45:14,461 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-10 13:45:14,579 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-10 13:45:14,579 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-10 13:45:14,860 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-10 13:45:14,918 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 1158@dark-robos-mbp
2017-09-10 13:45:14,930 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-10 13:45:14,931 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-10 13:45:14,934 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-10 13:45:14,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-10 13:45:14,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-10 13:45:14,978 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-10 13:45:14,980 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 10 13:45:14
2017-09-10 13:45:14,982 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-10 13:45:14,982 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-10 13:45:14,984 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-10 13:45:14,984 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-10 13:45:15,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-10 13:45:15,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-10 13:45:15,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-10 13:45:15,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-10 13:45:15,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-10 13:45:15,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-10 13:45:15,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-10 13:45:15,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-10 13:45:15,031 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-10 13:45:15,031 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-10 13:45:15,031 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-10 13:45:15,031 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-10 13:45:15,033 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-10 13:45:15,339 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-10 13:45:15,339 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-10 13:45:15,339 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-10 13:45:15,339 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-10 13:45:15,341 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-10 13:45:15,341 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-10 13:45:15,341 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-10 13:45:15,350 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-10 13:45:15,350 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-10 13:45:15,350 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-10 13:45:15,350 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-10 13:45:15,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-10 13:45:15,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-10 13:45:15,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-10 13:45:15,356 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-10 13:45:15,357 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-10 13:45:15,357 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-10 13:45:15,416 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-10 13:45:15,416 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-10 13:45:15,424 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-10 13:45:15,504 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-10 13:45:15,526 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-10 13:45:15,533 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-10 13:45:15,541 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-10 13:45:15,544 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-10 13:45:15,544 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-10 13:45:15,544 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-10 13:45:15,592 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-10 13:45:15,592 INFO org.mortbay.log: jetty-6.1.26
2017-09-10 13:45:15,870 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-10 13:45:15,871 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-10 13:46:17,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:18,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:19,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:20,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:21,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:22,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:23,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:24,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:25,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:26,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-10 13:46:26,119 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-10 13:46:26,131 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-10 13:46:59,336 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-10 13:46:59,340 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-10 13:47:58,828 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-10 13:47:58,855 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-10 13:47:59,432 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-10 13:47:59,601 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-10 13:47:59,706 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-10 13:47:59,706 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-10 13:47:59,932 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-10 13:47:59,952 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 2074@dark-robos-mbp
2017-09-10 13:47:59,962 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-10 13:47:59,963 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-10 13:47:59,966 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-10 13:48:00,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-10 13:48:00,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-10 13:48:00,015 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-10 13:48:00,017 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 10 13:48:00
2017-09-10 13:48:00,019 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-10 13:48:00,019 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-10 13:48:00,020 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-10 13:48:00,020 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-10 13:48:00,052 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-10 13:48:00,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-10 13:48:00,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-10 13:48:00,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-10 13:48:00,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-10 13:48:00,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-10 13:48:00,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-10 13:48:00,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-10 13:48:00,058 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-10 13:48:00,058 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-10 13:48:00,058 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-10 13:48:00,058 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-10 13:48:00,060 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-10 13:48:00,346 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-10 13:48:00,346 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-10 13:48:00,346 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-10 13:48:00,346 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-10 13:48:00,348 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-10 13:48:00,348 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-10 13:48:00,348 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-10 13:48:00,358 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-10 13:48:00,358 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-10 13:48:00,358 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-10 13:48:00,358 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-10 13:48:00,401 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-10 13:48:00,401 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-10 13:48:00,401 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-10 13:48:00,408 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-10 13:48:00,408 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-10 13:48:00,408 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-10 13:48:00,426 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-10 13:48:00,426 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-10 13:48:00,435 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-10 13:48:00,513 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-10 13:48:00,524 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-10 13:48:00,531 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-10 13:48:00,539 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-10 13:48:00,541 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-10 13:48:00,541 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-10 13:48:00,541 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-10 13:48:00,574 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-10 13:48:00,574 INFO org.mortbay.log: jetty-6.1.26
2017-09-10 13:48:00,813 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-10 13:48:00,813 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-10 14:13:56,944 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-10 14:13:56,952 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-12 14:57:35,205 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-12 14:57:35,250 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-12 14:57:35,828 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-12 14:57:36,011 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-12 14:57:36,123 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-12 14:57:36,123 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-12 14:57:36,444 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-12 14:57:36,507 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 1419@dark-robos-mbp
2017-09-12 14:57:36,517 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-12 14:57:36,518 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-12 14:57:36,522 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-12 14:57:36,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-12 14:57:36,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-12 14:57:36,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-12 14:57:36,573 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 12 14:57:36
2017-09-12 14:57:36,577 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-12 14:57:36,577 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 14:57:36,580 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-12 14:57:36,580 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-12 14:57:36,616 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-12 14:57:36,619 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-12 14:57:36,619 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-12 14:57:36,619 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-12 14:57:36,620 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-12 14:57:36,620 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-12 14:57:36,620 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-12 14:57:36,620 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-12 14:57:36,622 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-12 14:57:36,622 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-12 14:57:36,623 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-12 14:57:36,623 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-12 14:57:36,625 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-12 14:57:36,930 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-12 14:57:36,930 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 14:57:36,930 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-12 14:57:36,930 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-12 14:57:36,933 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-12 14:57:36,933 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-12 14:57:36,934 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-12 14:57:36,947 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-12 14:57:36,947 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 14:57:36,947 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-12 14:57:36,947 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-12 14:57:36,986 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-12 14:57:36,986 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-12 14:57:36,986 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-12 14:57:36,990 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-12 14:57:36,990 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-12 14:57:36,990 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-12 14:57:37,053 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-12 14:57:37,053 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-12 14:57:37,063 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-12 14:57:37,138 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-12 14:57:37,149 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-12 14:57:37,155 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-12 14:57:37,163 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-12 14:57:37,166 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-12 14:57:37,167 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-12 14:57:37,167 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-12 14:57:37,201 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-12 14:57:37,202 INFO org.mortbay.log: jetty-6.1.26
2017-09-12 14:57:37,411 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-12 14:57:37,411 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-12 14:58:38,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:39,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:40,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:41,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:42,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:43,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:44,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:45,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:46,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:47,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:58:47,675 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 14:58:47,686 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 14:59:48,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:49,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:50,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:51,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:52,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:53,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:54,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:55,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:56,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:57,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 14:59:57,758 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 14:59:58,100 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:00:59,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:00,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:01,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:02,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:03,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:04,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:05,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:06,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:07,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:08,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:01:08,180 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:01:08,183 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:02:09,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:10,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:11,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:12,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:13,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:14,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:15,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:16,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:17,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:18,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:02:18,241 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:02:18,656 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:03:19,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:20,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:21,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:22,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:23,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:24,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:25,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:26,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:27,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:28,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:03:28,710 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:03:28,712 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:04:29,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:30,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:31,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:32,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:33,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:34,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:35,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:36,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:37,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:38,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:04:38,758 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:04:38,763 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:05:39,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:40,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:41,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:42,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:43,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:44,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:45,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:46,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:47,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:48,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:05:48,846 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:05:49,265 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:06:50,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:51,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:52,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:53,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:54,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:55,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:56,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:57,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:58,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:59,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:06:59,339 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:06:59,342 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:08:00,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:01,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:02,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:03,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:04,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:05,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:06,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:07,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:08,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:09,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:08:09,582 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:08:09,587 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:09:10,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:11,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:12,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:13,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:14,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:15,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:16,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:17,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:18,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:19,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:09:19,647 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:09:19,989 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:10:21,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:22,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:23,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:24,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:25,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:26,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:27,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:28,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:29,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:30,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:10:30,041 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:10:30,045 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:11:31,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:32,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:33,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:34,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:35,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:36,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:37,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:38,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:39,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:40,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:11:40,095 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:11:40,905 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:12:41,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:42,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:43,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:44,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:45,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:46,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:47,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:48,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:49,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:50,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:12:50,971 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:12:50,974 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:13:52,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:13:53,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:13:54,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:13:55,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:13:56,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:13:57,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:13:58,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:13:59,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:14:00,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:14:01,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:14:01,092 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:14:01,093 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:15:02,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:03,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:04,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:05,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:06,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:07,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:08,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:09,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:10,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:11,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:15:11,168 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:15:11,722 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:16:12,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:13,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:14,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:15,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:16,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:17,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:18,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:19,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:20,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:21,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:16:21,786 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:16:21,789 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2017-09-12 15:17:22,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:23,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:24,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:25,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:26,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:27,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:28,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:29,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:30,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:31,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:17:31,844 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:17:32,252 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:18:33,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:34,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:35,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:36,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:37,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:38,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:39,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:40,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:41,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:42,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:18:42,298 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:18:42,300 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:19:43,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:44,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:45,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:46,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:47,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:48,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:49,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:50,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:51,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:52,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:19:52,337 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:19:52,338 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:20:53,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:20:54,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:20:55,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:20:56,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:20:57,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:20:58,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:20:59,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:21:00,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:21:01,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:21:02,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:21:02,434 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:21:02,847 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:22:03,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:04,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:05,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:06,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:07,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:08,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:09,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:10,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:11,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:12,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:22:12,906 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:22:12,908 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:23:13,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:14,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:15,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:16,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:17,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:18,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:19,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:20,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:21,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:22,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:23:22,959 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:23:22,961 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:24:23,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:24,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:25,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:26,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:27,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:28,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:29,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:31,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:32,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:33,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:24:33,010 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:24:33,013 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:25:34,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:35,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:36,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:37,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:38,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:39,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:40,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:41,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:42,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:43,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:25:43,075 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:25:43,078 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:26:44,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:45,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:46,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:47,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:48,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:49,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:50,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:51,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:52,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:53,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:26:53,125 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:26:53,539 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:27:54,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:27:55,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:27:56,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:27:57,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:27:58,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:27:59,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:28:00,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:28:01,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:28:02,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:28:03,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:28:03,584 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:28:03,587 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:29:04,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:05,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:06,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:07,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:08,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:09,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:10,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:11,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:12,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:13,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:29:13,642 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:29:13,645 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:30:14,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:15,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:16,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:17,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:18,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:19,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:20,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:21,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:22,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:23,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:30:23,705 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:30:23,709 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:31:24,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:25,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:26,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:27,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:28,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:29,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:30,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:31,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:32,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:33,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:31:33,754 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:31:33,757 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:32:34,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:35,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:36,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:37,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:38,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:39,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:40,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:41,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:42,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:43,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:32:43,802 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:32:44,217 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:33:45,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:46,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:47,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:48,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:49,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:50,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:51,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:52,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:53,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:54,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:33:54,268 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:33:54,271 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:34:55,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:34:56,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:34:57,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:34:58,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:34:59,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:35:00,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:35:01,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:35:02,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:35:03,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:35:04,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-09-12 15:35:04,328 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
2017-09-12 15:35:04,330 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From dark-robos-mbp/192.168.1.171 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2017-09-12 15:35:28,407 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-12 15:35:28,412 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-12 15:36:01,071 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-12 15:36:01,103 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-12 15:36:01,738 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-12 15:36:01,921 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-12 15:36:02,439 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-12 15:36:02,439 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-12 15:36:02,660 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-12 15:36:02,693 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 4380@dark-robos-mbp
2017-09-12 15:36:02,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-12 15:36:02,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-12 15:36:02,710 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-12 15:36:02,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-12 15:36:02,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-12 15:36:02,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-12 15:36:02,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 12 15:36:02
2017-09-12 15:36:02,760 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-12 15:36:02,760 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:36:02,761 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-12 15:36:02,761 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-12 15:36:02,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-12 15:36:02,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-12 15:36:02,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-12 15:36:02,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-12 15:36:02,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-12 15:36:02,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-12 15:36:02,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-12 15:36:02,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-12 15:36:02,800 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-12 15:36:02,800 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-12 15:36:02,800 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-12 15:36:02,801 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-12 15:36:02,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-12 15:36:03,108 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-12 15:36:03,108 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:36:03,108 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-12 15:36:03,108 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-12 15:36:03,110 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-12 15:36:03,110 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-12 15:36:03,110 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-12 15:36:03,120 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-12 15:36:03,120 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:36:03,120 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-12 15:36:03,120 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-12 15:36:03,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-12 15:36:03,123 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-12 15:36:03,123 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-12 15:36:03,127 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-12 15:36:03,127 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-12 15:36:03,127 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-12 15:36:03,138 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-12 15:36:03,138 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-12 15:36:03,148 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-12 15:36:03,223 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-12 15:36:03,234 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-12 15:36:03,241 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-12 15:36:03,249 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-12 15:36:03,252 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-12 15:36:03,252 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-12 15:36:03,252 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-12 15:36:03,286 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-12 15:36:03,286 INFO org.mortbay.log: jetty-6.1.26
2017-09-12 15:36:03,552 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-12 15:36:03,552 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-12 15:36:32,238 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-12 15:36:32,243 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-12 15:44:03,768 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-12 15:44:03,821 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-12 15:44:04,507 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-12 15:44:04,709 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-12 15:44:05,220 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-12 15:44:05,220 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-12 15:44:05,438 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-12 15:44:05,456 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 5030@dark-robos-mbp
2017-09-12 15:44:05,466 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-12 15:44:05,468 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-12 15:44:05,470 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-12 15:44:05,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-12 15:44:05,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-12 15:44:05,517 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-12 15:44:05,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 12 15:44:05
2017-09-12 15:44:05,521 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-12 15:44:05,521 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:44:05,522 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-12 15:44:05,523 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-12 15:44:05,560 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-12 15:44:05,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-12 15:44:05,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-12 15:44:05,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-12 15:44:05,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-12 15:44:05,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-12 15:44:05,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-12 15:44:05,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-12 15:44:05,566 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-12 15:44:05,566 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-12 15:44:05,566 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-12 15:44:05,567 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-12 15:44:05,569 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-12 15:44:05,872 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-12 15:44:05,872 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:44:05,873 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-12 15:44:05,873 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-12 15:44:05,874 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-12 15:44:05,874 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-12 15:44:05,875 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-12 15:44:05,883 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-12 15:44:05,884 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:44:05,884 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-12 15:44:05,884 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-12 15:44:05,886 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-12 15:44:05,886 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-12 15:44:05,886 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-12 15:44:05,890 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-12 15:44:05,890 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-12 15:44:05,890 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-12 15:44:05,902 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-12 15:44:05,902 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-12 15:44:05,915 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-12 15:44:05,992 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-12 15:44:06,003 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-12 15:44:06,010 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-12 15:44:06,018 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-12 15:44:06,021 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-12 15:44:06,021 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-12 15:44:06,021 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-12 15:44:06,056 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-12 15:44:06,056 INFO org.mortbay.log: jetty-6.1.26
2017-09-12 15:44:06,333 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-12 15:44:06,333 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-12 15:44:33,515 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-12 15:44:33,518 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-12 15:46:12,163 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-12 15:46:12,235 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-12 15:46:12,877 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-12 15:46:13,063 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-12 15:46:13,586 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-12 15:46:13,586 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-12 15:46:13,804 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-12 15:46:13,823 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 5614@dark-robos-mbp
2017-09-12 15:46:13,832 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-12 15:46:13,833 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-12 15:46:13,836 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-12 15:46:13,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-12 15:46:13,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-12 15:46:13,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-12 15:46:13,882 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 12 15:46:13
2017-09-12 15:46:13,884 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-12 15:46:13,884 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:46:13,885 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-12 15:46:13,886 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-12 15:46:13,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-12 15:46:13,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-12 15:46:13,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-12 15:46:13,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-12 15:46:13,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-12 15:46:13,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-12 15:46:13,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-12 15:46:13,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-12 15:46:13,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-12 15:46:13,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-12 15:46:13,938 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-12 15:46:13,938 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-12 15:46:13,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-12 15:46:14,245 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-12 15:46:14,245 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:46:14,245 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-12 15:46:14,246 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-12 15:46:14,248 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-12 15:46:14,248 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-12 15:46:14,248 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-12 15:46:14,257 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-12 15:46:14,257 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:46:14,257 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-12 15:46:14,257 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-12 15:46:14,260 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-12 15:46:14,260 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-12 15:46:14,260 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-12 15:46:14,264 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-12 15:46:14,265 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-12 15:46:14,265 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-12 15:46:14,277 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-12 15:46:14,277 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-12 15:46:14,287 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-12 15:46:14,370 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-12 15:46:14,382 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-12 15:46:14,388 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-12 15:46:14,397 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-12 15:46:14,400 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-12 15:46:14,401 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-12 15:46:14,401 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-12 15:46:14,434 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-12 15:46:14,434 INFO org.mortbay.log: jetty-6.1.26
2017-09-12 15:46:14,718 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-12 15:46:14,718 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-12 15:46:46,601 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-12 15:46:46,604 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-12 15:49:43,087 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-12 15:49:43,121 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-12 15:49:43,830 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-12 15:49:44,028 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-12 15:49:44,524 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-12 15:49:44,525 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-12 15:49:44,747 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-12 15:49:44,773 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 6203@dark-robos-mbp
2017-09-12 15:49:44,782 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-12 15:49:44,783 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-12 15:49:44,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-12 15:49:44,836 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-12 15:49:44,836 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-12 15:49:44,838 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-12 15:49:44,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 12 15:49:44
2017-09-12 15:49:44,843 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-12 15:49:44,843 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:49:44,844 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-12 15:49:44,844 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-12 15:49:44,884 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-12 15:49:44,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-12 15:49:44,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-12 15:49:44,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-12 15:49:44,888 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-12 15:49:44,888 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-12 15:49:44,888 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-12 15:49:44,888 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-12 15:49:44,890 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-12 15:49:44,890 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-12 15:49:44,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-12 15:49:44,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-12 15:49:44,894 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-12 15:49:45,226 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-12 15:49:45,227 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:49:45,227 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-12 15:49:45,227 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-12 15:49:45,229 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-12 15:49:45,229 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-12 15:49:45,230 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-12 15:49:45,243 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-12 15:49:45,244 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:49:45,244 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-12 15:49:45,244 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-12 15:49:45,247 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-12 15:49:45,247 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-12 15:49:45,247 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-12 15:49:45,251 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-12 15:49:45,251 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-12 15:49:45,251 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-12 15:49:45,275 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-12 15:49:45,275 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-12 15:49:45,289 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-12 15:49:45,371 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-12 15:49:45,383 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-12 15:49:45,394 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-12 15:49:45,401 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-12 15:49:45,405 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-12 15:49:45,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-12 15:49:45,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-12 15:49:45,440 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-12 15:49:45,440 INFO org.mortbay.log: jetty-6.1.26
2017-09-12 15:49:45,728 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-12 15:49:45,728 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-12 15:50:11,695 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-12 15:50:11,699 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-12 15:51:39,275 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-12 15:51:39,300 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-12 15:51:39,917 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-12 15:51:40,082 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-12 15:51:40,189 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-12 15:51:40,189 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-12 15:51:40,404 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-12 15:51:40,423 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 6812@dark-robos-mbp
2017-09-12 15:51:40,434 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-12 15:51:40,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-12 15:51:40,438 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-12 15:51:40,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-12 15:51:40,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-12 15:51:40,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-12 15:51:40,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 12 15:51:40
2017-09-12 15:51:40,494 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-12 15:51:40,494 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:51:40,496 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-12 15:51:40,496 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-12 15:51:40,527 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-12 15:51:40,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-12 15:51:40,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-12 15:51:40,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-12 15:51:40,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-12 15:51:40,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-12 15:51:40,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-12 15:51:40,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-12 15:51:40,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-12 15:51:40,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-12 15:51:40,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-12 15:51:40,533 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-12 15:51:40,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-12 15:51:40,856 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-12 15:51:40,856 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:51:40,857 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-12 15:51:40,857 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-12 15:51:40,859 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-12 15:51:40,859 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-12 15:51:40,859 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-12 15:51:40,869 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-12 15:51:40,870 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:51:40,870 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-12 15:51:40,870 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-12 15:51:40,911 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-12 15:51:40,911 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-12 15:51:40,911 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-12 15:51:40,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-12 15:51:40,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-12 15:51:40,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-12 15:51:40,928 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-12 15:51:40,930 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-12 15:51:40,940 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-12 15:51:41,017 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-12 15:51:41,028 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-12 15:51:41,035 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-12 15:51:41,043 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-12 15:51:41,046 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-12 15:51:41,046 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-12 15:51:41,046 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-12 15:51:41,081 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-12 15:51:41,081 INFO org.mortbay.log: jetty-6.1.26
2017-09-12 15:51:41,298 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-12 15:51:41,298 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-12 15:52:42,086 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-09-12 15:52:42,460 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:917204981:1505249479256:CID-f6009f65-f832-4528-a71a-5819710a8f94&bootstrapstandby=false
2017-09-12 15:52:42,547 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-09-12 15:52:42,965 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2017-09-12 15:52:42,965 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 325 bytes.
2017-09-12 15:52:42,972 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-63:917204981:1505249479256:CID-f6009f65-f832-4528-a71a-5819710a8f94
2017-09-12 15:52:42,987 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-09-12 15:52:42,987 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000004039421 size 0 bytes.
2017-09-12 15:52:43,085 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-09-12 15:52:43,131 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-09-12 15:52:43,132 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage_0000000000000000000
2017-09-12 15:52:43,132 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-09-12 15:52:43,152 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-09-12 15:52:43,159 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2017-09-12 15:52:43,160 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2017-09-12 15:52:43,203 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2017-09-12 15:52:43,210 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 using no compression
2017-09-12 15:52:43,252 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 of size 325 bytes saved in 0 seconds.
2017-09-12 15:52:43,257 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-DarkRobo/dfs/namesecondary
2017-09-12 15:52:43,264 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-DarkRobo/dfs/namesecondary
2017-09-12 15:52:43,348 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.08 seconds
2017-09-12 15:52:43,349 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 325
2017-09-12 15:57:40,028 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-12 15:57:40,046 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-12 15:59:08,898 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-12 15:59:08,930 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-12 15:59:09,534 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-12 15:59:09,712 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-12 15:59:09,837 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-12 15:59:09,837 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-12 15:59:10,056 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-12 15:59:10,078 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 8072@dark-robos-mbp
2017-09-12 15:59:10,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-12 15:59:10,136 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-12 15:59:10,140 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-12 15:59:10,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-12 15:59:10,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-12 15:59:10,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-12 15:59:10,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 12 15:59:10
2017-09-12 15:59:10,197 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-12 15:59:10,197 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:59:10,199 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-12 15:59:10,199 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-12 15:59:10,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-12 15:59:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-12 15:59:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-12 15:59:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-12 15:59:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-12 15:59:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-12 15:59:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-12 15:59:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-12 15:59:10,239 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-12 15:59:10,239 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-12 15:59:10,239 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-12 15:59:10,240 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-12 15:59:10,242 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-12 15:59:10,542 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-12 15:59:10,542 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:59:10,543 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-12 15:59:10,543 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-12 15:59:10,544 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-12 15:59:10,545 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-12 15:59:10,545 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-12 15:59:10,555 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-12 15:59:10,555 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-12 15:59:10,555 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-12 15:59:10,555 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-12 15:59:10,595 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-12 15:59:10,595 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-12 15:59:10,595 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-12 15:59:10,600 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-12 15:59:10,600 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-12 15:59:10,600 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-12 15:59:10,612 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-12 15:59:10,615 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-12 15:59:10,626 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-12 15:59:10,705 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-12 15:59:10,716 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-12 15:59:10,723 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-12 15:59:10,730 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-12 15:59:10,732 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-12 15:59:10,733 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-12 15:59:10,733 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-12 15:59:10,767 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-12 15:59:10,767 INFO org.mortbay.log: jetty-6.1.26
2017-09-12 15:59:11,004 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-12 15:59:11,004 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-12 15:59:36,543 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-12 15:59:36,546 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-17 16:34:12,655 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-17 16:34:12,743 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-17 16:34:13,732 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-17 16:34:13,920 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-17 16:34:14,030 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-17 16:34:14,030 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-17 16:34:16,046 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-17 16:34:16,077 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 994@dark-robos-mbp
2017-09-17 16:34:16,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-17 16:34:16,093 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-17 16:34:16,096 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-17 16:34:16,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-17 16:34:16,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-17 16:34:16,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-17 16:34:16,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 17 16:34:16
2017-09-17 16:34:16,190 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-17 16:34:16,191 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-17 16:34:16,192 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-17 16:34:16,192 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-17 16:34:16,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-17 16:34:16,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-17 16:34:16,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-17 16:34:16,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-17 16:34:16,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-17 16:34:16,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-17 16:34:16,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-17 16:34:16,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-17 16:34:16,292 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-17 16:34:16,292 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-17 16:34:16,296 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-17 16:34:16,297 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-17 16:34:16,300 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-17 16:34:17,200 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-17 16:34:17,200 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-17 16:34:17,200 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-17 16:34:17,200 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-17 16:34:17,205 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-17 16:34:17,205 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-17 16:34:17,207 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-17 16:34:17,217 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-17 16:34:17,217 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-17 16:34:17,218 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-17 16:34:17,218 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-17 16:34:17,300 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-17 16:34:17,300 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-17 16:34:17,300 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-17 16:34:17,305 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-17 16:34:17,305 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-17 16:34:17,305 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-17 16:34:17,449 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-17 16:34:17,449 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-17 16:34:17,490 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-17 16:34:17,637 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-17 16:34:17,665 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-17 16:34:17,672 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-17 16:34:17,684 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-17 16:34:17,687 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-17 16:34:17,687 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-17 16:34:17,687 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-17 16:34:17,815 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-17 16:34:17,815 INFO org.mortbay.log: jetty-6.1.26
2017-09-17 16:34:18,248 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-17 16:34:18,259 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-17 16:46:56,205 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-17 16:46:56,211 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-24 15:12:52,862 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-24 15:12:52,984 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-24 15:12:54,201 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-24 15:12:54,391 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-24 15:12:54,492 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-24 15:12:54,492 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-24 15:12:54,775 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-24 15:12:54,799 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 2682@dark-robos-mbp
2017-09-24 15:12:54,809 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-24 15:12:54,810 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-24 15:12:54,814 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-24 15:12:54,860 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-24 15:12:54,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-24 15:12:54,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-24 15:12:54,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 24 15:12:54
2017-09-24 15:12:54,869 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-24 15:12:54,869 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:12:54,871 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-24 15:12:54,871 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-24 15:12:54,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-24 15:12:54,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-24 15:12:54,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-24 15:12:54,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-24 15:12:54,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-24 15:12:54,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-24 15:12:54,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-24 15:12:54,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-24 15:12:54,916 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-24 15:12:54,916 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-24 15:12:54,916 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-24 15:12:54,917 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-24 15:12:54,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-24 15:12:55,279 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-24 15:12:55,280 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:12:55,280 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-24 15:12:55,280 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-24 15:12:55,282 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-24 15:12:55,282 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-24 15:12:55,283 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-24 15:12:55,292 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-24 15:12:55,292 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:12:55,292 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-24 15:12:55,292 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-24 15:12:55,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-24 15:12:55,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-24 15:12:55,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-24 15:12:55,337 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-24 15:12:55,337 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-24 15:12:55,337 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-24 15:12:55,355 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-24 15:12:55,355 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-24 15:12:55,364 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-24 15:12:55,441 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-24 15:12:55,451 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-24 15:12:55,457 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-24 15:12:55,464 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-24 15:12:55,468 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-24 15:12:55,468 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-24 15:12:55,469 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-24 15:12:55,509 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-24 15:12:55,509 INFO org.mortbay.log: jetty-6.1.26
2017-09-24 15:12:55,871 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-24 15:12:55,871 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-24 15:13:56,571 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-09-24 15:13:57,000 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=133&storageInfo=-63:917204981:1505249479256:CID-f6009f65-f832-4528-a71a-5819710a8f94&bootstrapstandby=false
2017-09-24 15:13:57,099 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-09-24 15:13:57,373 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 500.00 KB/s
2017-09-24 15:13:57,373 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000133 size 3551 bytes.
2017-09-24 15:13:57,379 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=134&endTxId=135&storageInfo=-63:917204981:1505249479256:CID-f6009f65-f832-4528-a71a-5819710a8f94
2017-09-24 15:13:57,404 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-09-24 15:13:57,404 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000134-0000000000000000135_0000000000008665611 size 0 bytes.
2017-09-24 15:13:57,474 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 47 INodes.
2017-09-24 15:13:57,533 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-09-24 15:13:57,533 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 133 from /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage_0000000000000000133
2017-09-24 15:13:57,534 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-09-24 15:13:57,543 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-09-24 15:13:57,548 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000134-0000000000000000135 expecting start txid #134
2017-09-24 15:13:57,549 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000134-0000000000000000135
2017-09-24 15:13:57,572 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000134-0000000000000000135 of size 42 edits # 2 loaded in 0 seconds
2017-09-24 15:13:57,577 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000135 using no compression
2017-09-24 15:13:57,623 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000135 of size 3551 bytes saved in 0 seconds.
2017-09-24 15:13:57,626 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-DarkRobo/dfs/namesecondary
2017-09-24 15:13:57,635 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-DarkRobo/dfs/namesecondary
2017-09-24 15:13:57,765 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 135 to namenode at http://localhost:50070 in 0.125 seconds
2017-09-24 15:13:57,766 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 3551
2017-09-24 15:15:28,086 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-24 15:15:28,108 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-24 15:16:34,663 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-24 15:16:34,723 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-24 15:16:35,370 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-24 15:16:35,544 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-24 15:16:36,066 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-24 15:16:36,067 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-24 15:16:36,281 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-24 15:16:36,299 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 3578@dark-robos-mbp
2017-09-24 15:16:36,342 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-24 15:16:36,343 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-24 15:16:36,346 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-24 15:16:36,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-24 15:16:36,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-24 15:16:36,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-24 15:16:36,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 24 15:16:36
2017-09-24 15:16:36,397 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-24 15:16:36,397 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:16:36,399 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-24 15:16:36,399 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-24 15:16:36,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-24 15:16:36,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-24 15:16:36,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-24 15:16:36,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-24 15:16:36,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-24 15:16:36,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-24 15:16:36,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-24 15:16:36,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-24 15:16:36,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-24 15:16:36,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-24 15:16:36,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-24 15:16:36,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-24 15:16:36,443 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-24 15:16:36,733 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-24 15:16:36,733 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:16:36,733 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-24 15:16:36,733 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-24 15:16:36,735 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-24 15:16:36,735 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-24 15:16:36,735 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-24 15:16:36,745 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-24 15:16:36,746 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:16:36,746 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-24 15:16:36,746 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-24 15:16:36,748 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-24 15:16:36,748 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-24 15:16:36,748 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-24 15:16:36,752 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-24 15:16:36,752 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-24 15:16:36,752 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-24 15:16:36,764 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-24 15:16:36,764 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-24 15:16:36,773 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-24 15:16:36,851 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-24 15:16:36,862 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-24 15:16:36,869 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-24 15:16:36,877 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-24 15:16:36,879 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-24 15:16:36,879 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-24 15:16:36,880 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-24 15:16:36,915 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-24 15:16:36,915 INFO org.mortbay.log: jetty-6.1.26
2017-09-24 15:16:37,221 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-24 15:16:37,221 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-24 15:17:37,631 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-09-24 15:17:37,756 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=135&storageInfo=-63:917204981:1505249479256:CID-f6009f65-f832-4528-a71a-5819710a8f94&bootstrapstandby=false
2017-09-24 15:17:37,821 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-09-24 15:17:38,019 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 600.00 KB/s
2017-09-24 15:17:38,019 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000135 size 3551 bytes.
2017-09-24 15:17:38,027 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=136&endTxId=136&storageInfo=-63:917204981:1505249479256:CID-f6009f65-f832-4528-a71a-5819710a8f94
2017-09-24 15:17:38,059 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 36571.43 KB/s
2017-09-24 15:17:38,059 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000136-0000000000000000136_0000000000008886259 size 0 bytes.
2017-09-24 15:17:38,060 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=137&endTxId=138&storageInfo=-63:917204981:1505249479256:CID-f6009f65-f832-4528-a71a-5819710a8f94
2017-09-24 15:17:38,067 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-09-24 15:17:38,067 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000137-0000000000000000138_0000000000008886292 size 0 bytes.
2017-09-24 15:17:38,136 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 47 INodes.
2017-09-24 15:17:38,200 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-09-24 15:17:38,200 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 135 from /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage_0000000000000000135
2017-09-24 15:17:38,201 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-09-24 15:17:38,207 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2017-09-24 15:17:38,212 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000136-0000000000000000136 expecting start txid #136
2017-09-24 15:17:38,213 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000136-0000000000000000136
2017-09-24 15:17:38,236 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000136-0000000000000000136 of size 1048576 edits # 1 loaded in 0 seconds
2017-09-24 15:17:38,236 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000137-0000000000000000138 expecting start txid #137
2017-09-24 15:17:38,236 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000137-0000000000000000138
2017-09-24 15:17:38,236 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/edits_0000000000000000137-0000000000000000138 of size 42 edits # 2 loaded in 0 seconds
2017-09-24 15:17:38,241 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000138 using no compression
2017-09-24 15:17:38,288 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage.ckpt_0000000000000000138 of size 3551 bytes saved in 0 seconds.
2017-09-24 15:17:38,294 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 135
2017-09-24 15:17:38,294 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-DarkRobo/dfs/namesecondary/current/fsimage_0000000000000000133, cpktTxId=0000000000000000133)
2017-09-24 15:17:38,361 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 138 to namenode at http://localhost:50070 in 0.056 seconds
2017-09-24 15:17:38,361 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 3551
2017-09-24 15:30:34,377 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-24 15:30:34,394 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-24 15:38:02,966 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-24 15:38:03,006 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-24 15:38:03,587 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-24 15:38:03,755 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-24 15:38:03,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-24 15:38:03,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-24 15:38:04,151 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-24 15:38:04,173 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 931@dark-robos-mbp
2017-09-24 15:38:04,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-24 15:38:04,183 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-24 15:38:04,186 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-24 15:38:04,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-24 15:38:04,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-24 15:38:04,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-24 15:38:04,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 24 15:38:04
2017-09-24 15:38:04,238 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-24 15:38:04,238 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:38:04,240 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-24 15:38:04,240 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-24 15:38:04,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-24 15:38:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-24 15:38:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-24 15:38:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-24 15:38:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-24 15:38:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-24 15:38:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-24 15:38:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-24 15:38:04,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-24 15:38:04,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-24 15:38:04,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-24 15:38:04,319 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-24 15:38:04,321 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-24 15:38:04,630 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-24 15:38:04,630 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:38:04,630 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-24 15:38:04,631 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-24 15:38:04,633 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-24 15:38:04,633 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-24 15:38:04,633 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-24 15:38:04,643 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-24 15:38:04,643 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-24 15:38:04,643 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-24 15:38:04,643 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-24 15:38:04,681 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-24 15:38:04,681 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-24 15:38:04,681 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-24 15:38:04,685 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-24 15:38:04,685 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-24 15:38:04,685 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-24 15:38:04,737 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-24 15:38:04,737 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-24 15:38:04,748 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-24 15:38:04,825 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-24 15:38:04,841 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-24 15:38:04,849 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-24 15:38:04,856 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-24 15:38:04,859 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-24 15:38:04,859 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-24 15:38:04,859 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-24 15:38:04,894 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-24 15:38:04,894 INFO org.mortbay.log: jetty-6.1.26
2017-09-24 15:38:05,110 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-24 15:38:05,110 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-09-24 15:42:40,472 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-09-24 15:42:40,485 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at dark-robos-mbp/192.168.1.171
************************************************************/
2017-09-29 12:56:54,575 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = DarkRobo
STARTUP_MSG:   host = dark-robos-mbp/192.168.1.171
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /Users/DarkRobo/hadoop-2.8.1/etc/hadoop:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/Users/DarkRobo/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_144
************************************************************/
2017-09-29 12:56:54,624 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-09-29 12:56:56,060 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-09-29 12:56:56,436 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-09-29 12:56:56,767 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-09-29 12:56:56,768 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-09-29 12:56:57,497 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2017-09-29 12:56:57,561 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-DarkRobo/dfs/namesecondary/in_use.lock acquired by nodename 1116@dark-robos-mbp
2017-09-29 12:56:57,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2017-09-29 12:56:57,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2017-09-29 12:56:57,596 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2017-09-29 12:56:57,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-09-29 12:56:57,764 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-09-29 12:56:57,766 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-09-29 12:56:57,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Sep 29 12:56:57
2017-09-29 12:56:57,789 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-09-29 12:56:57,789 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-29 12:56:57,791 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-09-29 12:56:57,791 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-09-29 12:56:57,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-09-29 12:56:57,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-09-29 12:56:57,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-09-29 12:56:57,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-09-29 12:56:57,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-09-29 12:56:57,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-09-29 12:56:57,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-09-29 12:56:57,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-09-29 12:56:57,890 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = DarkRobo (auth:SIMPLE)
2017-09-29 12:56:57,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-09-29 12:56:57,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-09-29 12:56:57,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-09-29 12:56:57,894 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-09-29 12:56:58,700 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-09-29 12:56:58,700 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-29 12:56:58,700 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-09-29 12:56:58,700 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-09-29 12:56:58,702 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-09-29 12:56:58,702 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-09-29 12:56:58,703 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2017-09-29 12:56:58,724 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-09-29 12:56:58,724 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-09-29 12:56:58,724 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-09-29 12:56:58,724 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-09-29 12:56:58,726 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-09-29 12:56:58,726 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-09-29 12:56:58,726 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-09-29 12:56:58,736 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-09-29 12:56:58,736 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-09-29 12:56:58,736 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-09-29 12:56:58,766 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-09-29 12:56:58,766 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-09-29 12:56:58,775 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-09-29 12:56:59,043 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-09-29 12:56:59,055 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-09-29 12:56:59,101 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-09-29 12:56:59,108 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-09-29 12:56:59,130 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-09-29 12:56:59,130 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-09-29 12:56:59,130 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-09-29 12:56:59,207 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-09-29 12:56:59,207 INFO org.mortbay.log: jetty-6.1.26
2017-09-29 12:57:00,260 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-09-29 12:57:00,271 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
